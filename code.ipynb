{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoRE13/Neonatal-Seizure-Detection/blob/main/REI505M_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XdlatiG9ed0"
      },
      "source": [
        "# Final Project - REI505M\n",
        "\n",
        "The following notebook contains all code used for the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3Gehb1WLkS7",
        "outputId": "df7ac033-b865-498c-a6a1-452d130b4b11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount the notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FOLDERNAME = 'Gervigreind/final-project/'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/{}'.format(FOLDERNAME))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UpupIY3l7wW"
      },
      "source": [
        "## Data Porcessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_seizure_len = np.zeros((79,3))\n",
        "for i in range(79):\n",
        "  annot = np.genfromtxt('drive/MyDrive/Gervigreind/final-project/neonat/annotation/id{:d}.txt'.format(i+1),\\\n",
        "                        delimiter='\\t',skip_header=False)\n",
        "  for j in range(3):\n",
        "    num_seiz_ep = 0\n",
        "    annotations = annot[:,j]\n",
        "    for k in range(len(annotations)-1):\n",
        "      if annotations[k] == 1 and annotations[k+1] == 0: # If there is a zero after a 1 a sizure eepisode has ended\n",
        "        num_seiz_ep += 1\n",
        "    if annotations[-1] == 1: # If ends in a seizure\n",
        "      num_seiz_ep += 1\n",
        "    if num_seiz_ep == 0:\n",
        "      avg_seizure_len[i,j] = 0\n",
        "    else:\n",
        "      avg_seizure_len[i,j] = np.sum(annotations) / num_seiz_ep\n",
        "\n",
        "print('Average duration of seizure episodes (expert A, expert B, expert C)\\n')\n",
        "for i in range(79):\n",
        "  print('Recording ' + str(i+1) + ': ' + str(round(avg_seizure_len[i,0],2))+'s ' + str(round(avg_seizure_len[i,1],2)) + 's ' + str(round(avg_seizure_len[i,2],2))+'s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrBgT83KK5Wu",
        "outputId": "980cd0d8-da7d-4b24-8932-aea176e8d9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average duration of seizure episodes (expert A, expert B, expert C)\n",
            "\n",
            "Recording 1: 64.08s 74.79s 27.69s\n",
            "Recording 2: 32.5s 0.0s 0.0s\n",
            "Recording 3: 0.0s 0.0s 0.0s\n",
            "Recording 4: 462.5s 165.86s 329.0s\n",
            "Recording 5: 665.0s 637.8s 454.86s\n",
            "Recording 6: 0.0s 0.0s 122.75s\n",
            "Recording 7: 104.17s 46.19s 101.33s\n",
            "Recording 8: 32.0s 36.5s 0.0s\n",
            "Recording 9: 294.0s 130.12s 287.33s\n",
            "Recording 10: 0.0s 0.0s 0.0s\n",
            "Recording 11: 33.0s 31.75s 40.0s\n",
            "Recording 12: 0.0s 0.0s 31.0s\n",
            "Recording 13: 254.2s 236.83s 224.33s\n",
            "Recording 14: 47.6s 92.81s 60.89s\n",
            "Recording 15: 77.42s 95.5s 65.4s\n",
            "Recording 16: 20.67s 94.38s 22.21s\n",
            "Recording 17: 42.75s 194.33s 20.0s\n",
            "Recording 18: 0.0s 0.0s 0.0s\n",
            "Recording 19: 244.89s 206.08s 218.2s\n",
            "Recording 20: 44.18s 94.42s 43.92s\n",
            "Recording 21: 43.0s 42.0s 39.0s\n",
            "Recording 22: 87.12s 229.83s 91.43s\n",
            "Recording 23: 141.5s 0.0s 14.11s\n",
            "Recording 24: 0.0s 295.0s 0.0s\n",
            "Recording 25: 23.67s 57.0s 18.25s\n",
            "Recording 26: 0.0s 0.0s 13.71s\n",
            "Recording 27: 0.0s 0.0s 0.0s\n",
            "Recording 28: 0.0s 0.0s 0.0s\n",
            "Recording 29: 0.0s 0.0s 0.0s\n",
            "Recording 30: 0.0s 0.0s 0.0s\n",
            "Recording 31: 91.0s 90.5s 84.0s\n",
            "Recording 32: 0.0s 0.0s 0.0s\n",
            "Recording 33: 582.0s 0.0s 14.83s\n",
            "Recording 34: 455.0s 452.0s 452.0s\n",
            "Recording 35: 0.0s 0.0s 0.0s\n",
            "Recording 36: 246.0s 532.0s 225.5s\n",
            "Recording 37: 0.0s 0.0s 0.0s\n",
            "Recording 38: 157.0s 281.56s 119.33s\n",
            "Recording 39: 376.5s 359.57s 371.5s\n",
            "Recording 40: 50.08s 34.83s 64.12s\n",
            "Recording 41: 179.58s 945.0s 147.28s\n",
            "Recording 42: 0.0s 0.0s 0.0s\n",
            "Recording 43: 0.0s 0.0s 14.0s\n",
            "Recording 44: 48.71s 39.7s 41.88s\n",
            "Recording 45: 0.0s 0.0s 0.0s\n",
            "Recording 46: 0.0s 0.0s 20.75s\n",
            "Recording 47: 70.33s 62.4s 67.0s\n",
            "Recording 48: 0.0s 0.0s 0.0s\n",
            "Recording 49: 0.0s 0.0s 0.0s\n",
            "Recording 50: 93.0s 89.8s 90.2s\n",
            "Recording 51: 96.25s 320.0s 61.38s\n",
            "Recording 52: 119.0s 50.0s 43.0s\n",
            "Recording 53: 0.0s 0.0s 0.0s\n",
            "Recording 54: 158.38s 0.0s 126.59s\n",
            "Recording 55: 0.0s 0.0s 0.0s\n",
            "Recording 56: 0.0s 0.0s 814.0s\n",
            "Recording 57: 0.0s 0.0s 0.0s\n",
            "Recording 58: 0.0s 0.0s 0.0s\n",
            "Recording 59: 0.0s 0.0s 0.0s\n",
            "Recording 60: 0.0s 0.0s 0.0s\n",
            "Recording 61: 0.0s 0.0s 42.67s\n",
            "Recording 62: 382.0s 390.0s 380.0s\n",
            "Recording 63: 74.6s 86.75s 81.6s\n",
            "Recording 64: 0.0s 68.2s 25.67s\n",
            "Recording 65: 0.0s 0.0s 23.5s\n",
            "Recording 66: 869.0s 939.0s 774.0s\n",
            "Recording 67: 89.0s 103.0s 82.95s\n",
            "Recording 68: 33.0s 38.0s 0.0s\n",
            "Recording 69: 162.0s 383.89s 181.07s\n",
            "Recording 70: 0.0s 0.0s 0.0s\n",
            "Recording 71: 31.0s 79.75s 53.5s\n",
            "Recording 72: 0.0s 0.0s 0.0s\n",
            "Recording 73: 144.67s 285.0s 149.57s\n",
            "Recording 74: 0.0s 87.6s 33.83s\n",
            "Recording 75: 920.0s 925.0s 918.0s\n",
            "Recording 76: 238.5s 200.0s 143.0s\n",
            "Recording 77: 258.0s 154.0s 173.33s\n",
            "Recording 78: 100.73s 114.23s 83.79s\n",
            "Recording 79: 39.6s 59.38s 40.33s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7OvPW1zHFVb"
      },
      "outputs": [],
      "source": [
        "def z_score_normalize(data):\n",
        "  \"\"\"\n",
        "  Standardices individual signals\n",
        "\n",
        "  inputs: data is a time series\n",
        "  \"\"\"\n",
        "  return (data - np.mean(data)) / np.std(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7fouMejARP0T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from neonat.utils.edf import read_edf\n",
        "from neonat.utils.filter_signal import filter_signal\n",
        "from scipy.signal import butter, filtfilt\n",
        "from scipy.signal import decimate\n",
        "import pickle\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def loadEdfDataset (number_of_recordings = 79, class_type='consensus'):\n",
        "  \"\"\"\n",
        "  Function that reads in the eeg files and their annotations and preproceesses the data.\n",
        "\n",
        "  Inputs: number_of_recordings is the number of eeg files\n",
        "          class_type is type of labeling that should be used\n",
        "\n",
        "  Outputs: recordings is a list containing n segments where each segment is a (21,512) array that is a 16 second eeg segment\n",
        "           y is a n long list containing the corresponding labels for seegment in recordings\n",
        "           number_samples_per_rec is the number of segments for each recording.\n",
        "  \"\"\"\n",
        "  recordings = []\n",
        "  y = []\n",
        "  number_of_samples_per_rec = np.zeros(number_of_recordings) # to sort the segments by recording\n",
        "\n",
        "  for recording_id in range(1,number_of_recordings+1):\n",
        "    file_name = 'drive/MyDrive/Gervigreind/final-project/neonat/eeg/eeg{}.edf'.format(recording_id)\n",
        "    sampling_freq, downsample, dsf, data, channel_labels, n, start_time, _ = read_edf(file_name, 256)\n",
        "    sampling_freq = int(sampling_freq)\n",
        "    annot = np.genfromtxt('drive/MyDrive/Gervigreind/final-project/neonat/annotation/id{:d}.txt'.format(recording_id),\\\n",
        "                        delimiter='\\t',skip_header=False)\n",
        "    duration = int(data.shape[1] / sampling_freq)\n",
        "\n",
        "    # Filter the data with band filter and downsample the data from 256 Hz sampling rate to 32 Hz i.e. a factor of 8\n",
        "    n_channels = int(data.shape[0])\n",
        "    n_samples = int(data.shape[1])\n",
        "    new_sampling_freq = 32\n",
        "    factor = 256/32\n",
        "\n",
        "    data_filtered = np.zeros((n_channels, int(n_samples/factor)))\n",
        "    for i in range(n_channels):\n",
        "      single_channel = data[i,:].reshape(1,-1)\n",
        "      data_filtered[i,:] = decimate(filter_signal(single_channel)[0,:], int(factor)) # filter signal\n",
        "      data_filtered[i,:] = z_score_normalize(data_filtered[i,:]) # standardize individual signal\n",
        "\n",
        "    sec = 0\n",
        "    while (4*sec+16 < duration):\n",
        "      segment = data_filtered[:, 4*sec*new_sampling_freq:(4*sec+16)*new_sampling_freq] #16 sec segment of recording\n",
        "      classA = np.sum(annot[4*sec: 4*sec+16, 0])\n",
        "      classB = np.sum(annot[4*sec: 4*sec+16, 1])\n",
        "      classC = np.sum(annot[4*sec: 4*sec+16, 2])\n",
        "      if ( classA + classB + classC == 0): # all classified as 0\n",
        "        recordings.append(segment)\n",
        "        y.append(0)\n",
        "        number_of_samples_per_rec[recording_id-1] += 1\n",
        "      if(class_type == 'consensus' and classA + classB + classC == 48): # consensus labeling\n",
        "        recordings.append(segment)\n",
        "        y.append(1)\n",
        "        number_of_samples_per_rec[recording_id-1] += 1\n",
        "      if(class_type == 'majority' and (classA + classB == 32 or classA + classC == 32 or classB + classC == 32)): # majority labeling\n",
        "        recordings.append(segment)\n",
        "        y.append(1)\n",
        "        number_of_samples_per_rec[recording_id-1] += 1\n",
        "      if(class_type == 'contains' and (classA == 16 or classB == 16 or classC == 16)): # contains labeling\n",
        "        recordings.append(segment)\n",
        "        y.append(1)\n",
        "        number_of_samples_per_rec[recording_id-1] += 1\n",
        "      sec = sec + 1\n",
        "\n",
        "  return recordings, y, number_of_samples_per_rec\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the dataset\n",
        "recordings_consensus, y_consensus, number_of_samples_per_rec_consensus = loadEdfDataset()\n",
        "#recordings_majority, y_majority, number_of_samples_per_rec_majority = loadEdfDataset(class_type='majority')\n",
        "#recordings_contains, y_contains, number_of_samples_per_rec_contains = loadEdfDataset(class_type='contains')"
      ],
      "metadata": {
        "id": "r5UtvxWLR_LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_seiz_cons = np.sum(y_consensus)\n",
        "pos_seiz_maj = np.sum(y_majority)\n",
        "pos_seiz_cont = np.sum(y_contains)\n",
        "neg_seiz_cons = len(y_consensus) - pos_seiz_cons\n",
        "neg_seiz_maj = len(y_majority) - pos_seiz_maj\n",
        "neg_seiz_cont = len(y_contains) - pos_seiz_cont\n",
        "\n",
        "print('Number of seizure segments in consensus {}'.format(pos_seiz_cons))\n",
        "print('Number of negative segments in consenus {}\\n'.format(neg_seiz_cons))\n",
        "\n",
        "print('Number of seizure segments in majority {}'.format(pos_seiz_maj))\n",
        "print('Number of negative segments in majority {}\\n'.format(neg_seiz_maj))\n",
        "\n",
        "print('Number of seizure segments in contains {}'.format(pos_seiz_cont))\n",
        "print('Number of negative segments in contains {}\\n'.format(neg_seiz_cont))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CABMCvGO99EP",
        "outputId": "fef5cd21-7f4c-4d4f-a231-8daab5f92620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of seizure segments in consensus 8563\n",
            "Number of negative segments in consenus 80106\n",
            "\n",
            "Number of seizure segments in majority 10847\n",
            "Number of negative segments in majority 80106\n",
            "\n",
            "Number of seizure segments in contains 16455\n",
            "Number of negative segments in contains 80106\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature classifier"
      ],
      "metadata": {
        "id": "m-Rnt2-rfHEC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rA-HRTJXaf9"
      },
      "outputs": [],
      "source": [
        "from neonat.pyeeg import hjorth_mobility_complexity as hmc\n",
        "from neonat.pyeeg import entropy\n",
        "from scipy.integrate import simps\n",
        "from scipy import signal\n",
        "\n",
        "# Calculate the Hjorth parameters\n",
        "\n",
        "def calc_hjorth_features(x):\n",
        "    \"\"\"\n",
        "    Calculates the Hjorth parameters for signal x\n",
        "\n",
        "    inputs: x contains EEG data from a single channel\n",
        "\n",
        "    output: Activity, Mobility, Complexity\n",
        "    \"\"\"\n",
        "    Mobility, Complexity = hmc.hjorth(x)\n",
        "    Activity = np.var(x)\n",
        "    return Activity, Mobility, Complexity\n",
        "\n",
        "\n",
        "def band_power(x, fs, low=0.5, high=4, win=1):\n",
        "    \"\"\"\n",
        "    Calculate the power spectrum of a signal\n",
        "    Input: x contains EEG data from a single channel\n",
        "            fs sampling rate\n",
        "            low, high are the frequency band limits\n",
        "            win is the window length\n",
        "    Output: Absolute and relative power in the [low,high] band\n",
        "\n",
        "    Code is based on: https://raphaelvallat.com/bandpower.html\n",
        "    \"\"\"\n",
        "\n",
        "    # Welch's averaged periodogram is based on the Fast Fourier Transform (FFT)\n",
        "    freqs, psd = signal.welch(x, fs, nperseg=win*fs)\n",
        "    idx_delta = np.logical_and(freqs >= low, freqs <= high)\n",
        "    freq_res = freqs[1] - freqs[0]\n",
        "\n",
        "    # Compute the absolute power by approximating the area under the curve\n",
        "    abs_power = simps(psd[idx_delta], dx=freq_res)\n",
        "    #print('Absolute delta power: %.3f uV^2' % abs_power)\n",
        "\n",
        "    # Relative delta power (expressed as a percentage of total power)\n",
        "    total_power = simps(psd, dx=freq_res)\n",
        "    rel_power = abs_power / total_power\n",
        "    #print('Relative delta power: %.3f' % rel_power)\n",
        "    return abs_power, rel_power"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P3owfFoKXiw"
      },
      "outputs": [],
      "source": [
        "# Creating the feature based matrices\n",
        "def calc_features(recordings,y,class_type):\n",
        "  \"\"\"\n",
        "  Creates a feature based training set from the training data\n",
        "\n",
        "  Inputs: recordings are the raw eeg seegments\n",
        "          y are the corresponding labels\n",
        "          class_type is the kind of labeling that was used\n",
        "\n",
        "  Output: X is an (n,5) array where each row contains the features for corresponding segment of recordings\n",
        "  \"\"\"\n",
        "  sampling_freq = 32\n",
        "  X = np.zeros((len(recordings),5))\n",
        "  counter = 0\n",
        "  for segment in recordings:\n",
        "    hjort_features = np.apply_along_axis(calc_hjorth_features, arr=segment, axis=1)\n",
        "    activity = hjort_features[:,0]\n",
        "    mobility = hjort_features[:,1]\n",
        "    complexity = hjort_features[:,2]\n",
        "    band_power_1 = np.apply_along_axis(band_power, arr=segment, low=2, high=4, fs=sampling_freq, axis=1)\n",
        "    band_power_2 = np.apply_along_axis(band_power, arr=segment, low=4, high=6, fs=sampling_freq, axis=1)\n",
        "    X[counter] = [np.mean(activity), np.mean(mobility), np.mean(complexity), np.mean(band_power_1[:,0]), np.mean(band_power_2[:,0])]\n",
        "    counter += 1\n",
        "    if (counter % 1000 == 0):\n",
        "      print(counter)\n",
        "  with open('drive/MyDrive/Gervigreind/final-project/features-{}.pkl'.format(class_type), 'wb') as f:\n",
        "    pickle.dump(X, f)\n",
        "  with open('drive/MyDrive/Gervigreind/final-project/labels-{}.pkl'.format(class_type), 'wb') as f:\n",
        "    pickle.dump(y, f)\n",
        "  return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HZ45oNLTyvo"
      },
      "outputs": [],
      "source": [
        "def X_y_k_split(X,y, number_of_samples_per_rec,i, k):\n",
        "  \"\"\"\n",
        "  Returns the i-th split for k-fold cross validation\n",
        "\n",
        "  Inputs: X is the features array of the eeg segments\n",
        "          y contains the labels for X\n",
        "          number_of_samples_per_rec is the number of samples from each recording so we can split the data on a patient basis\n",
        "          i is thee fold number\n",
        "          k is the number of folds\n",
        "\n",
        "  Output: X_train, X_test are the train and test sets\n",
        "          y_train, y_test are the corresponding labels\n",
        "  \"\"\"\n",
        "  np.random.seed(21) # we always get the same shuffle\n",
        "  size = int(79/k)\n",
        "  testing_indiced = np.random.randint(0, 79, size=79)[i*size:(i+1)*size]\n",
        "\n",
        "  X_train = []\n",
        "  X_test = []\n",
        "  y_train = []\n",
        "  y_test = []\n",
        "\n",
        "  counter = 0\n",
        "  for i in range(79):\n",
        "    num = int(number_of_samples_per_rec[i])\n",
        "    if i in testing_indiced:\n",
        "      for j in range(num):\n",
        "        X_test.append(X[counter+j,:])\n",
        "        y_test.append(y[counter+j])\n",
        "    else:\n",
        "      for j in range(num):\n",
        "        X_train.append(X[counter+j,:])\n",
        "        y_train.append(y[counter+j])\n",
        "    counter += num\n",
        "\n",
        "  return X_train, X_test, y_train, y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b9yfdXCQAeg"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "def xgb_model(X,y,k,number_of_samples_per_rec):\n",
        "  \"\"\"\n",
        "  Train and tests a XGBBoost classifier with k-fold cross validation\n",
        "\n",
        "  Inputs: X and y are the features and labels matrices\n",
        "          k is the number of folds\n",
        "          number_of_samples_per_rec is the number of samples from each recording so we can split the data on a patient basis\n",
        "\n",
        "  Outputs: The output is the mean of the following metrics over the random splits:\n",
        "              precision\n",
        "              recall\n",
        "              f1-score\n",
        "              support\n",
        "              auc\n",
        "  \"\"\"\n",
        "  precision = np.zeros((2,k))\n",
        "  recall = np.zeros((2,k))\n",
        "  f1 = np.zeros((2,k))\n",
        "  support = np.zeros((2,k))\n",
        "  auc = np.zeros(k)\n",
        "\n",
        "  for i in range(k):\n",
        "    X_train, X_test, y_train, y_test = X_y_k_split(X,y,number_of_samples_per_rec,i,k)\n",
        "\n",
        "    num_positives = np.sum(y_train)\n",
        "    num_negatives = len(y_train) - num_positives\n",
        "    scale_pos_weight = num_negatives / num_positives\n",
        "\n",
        "    # default values\n",
        "    xgb_model = XGBClassifier(\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "    )\n",
        "\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    y_pred = xgb_model.predict(X_test)\n",
        "    c_rep = classification_report(y_test, y_pred, output_dict=True)\n",
        "    y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
        "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    precision[0,i] = c_rep['0']['precision']\n",
        "    precision[1,i] = c_rep['1']['precision']\n",
        "    recall[0,i] = c_rep['0']['recall']\n",
        "    recall[1,i] = c_rep['1']['recall']\n",
        "    f1[0,i] = c_rep['0']['f1-score']\n",
        "    f1[1,i] = c_rep['1']['f1-score']\n",
        "    support[0,i] = c_rep['0']['support']\n",
        "    support[1,i] = c_rep['1']['support']\n",
        "    auc[i] = auc_score\n",
        "\n",
        "  return np.mean(precision, axis=1), np.mean(recall, axis=1), np.mean(f1, axis=1), np.mean(support, axis=1), np.mean(auc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load features matrices\n",
        "with open('drive/MyDrive/Gervigreind/final-project/features-consensus.pkl', 'rb') as input_file:\n",
        "    X_consensus = pickle.load(input_file)\n",
        "with open('drive/MyDrive/Gervigreind/final-project/features-majority.pkl', 'rb') as input_file:\n",
        "    X_majority = pickle.load(input_file)\n",
        "with open('drive/MyDrive/Gervigreind/final-project/features-contains.pkl', 'rb') as input_file:\n",
        "    X_contains = pickle.load(input_file)\n",
        "\n",
        "# Load labels\n",
        "with open('drive/MyDrive/Gervigreind/final-project/labels-consensus.pkl', 'rb') as input_file:\n",
        "    y_consensus = pickle.load(input_file)\n",
        "with open('drive/MyDrive/Gervigreind/final-project/labels-majority.pkl', 'rb') as input_file:\n",
        "    y_majority = pickle.load(input_file)\n",
        "with open('drive/MyDrive/Gervigreind/final-project/labels-contains.pkl', 'rb') as input_file:\n",
        "    y_contains = pickle.load(input_file)\n",
        "\n",
        "# Load number of samples\n",
        "with open('drive/MyDrive/Gervigreind/final-project/number-samples-consensus.pkl', 'rb') as input_file:\n",
        "    number_of_samples_per_rec_cons = pickle.load(input_file)\n",
        "with open('drive/MyDrive/Gervigreind/final-project/number-samples-majority.pkl', 'rb') as input_file:\n",
        "    number_of_samples_per_rec_maj = pickle.load(input_file)\n",
        "with open('drive/MyDrive/Gervigreind/final-project/number-samples-contains.pkl', 'rb') as input_file:\n",
        "    number_of_samples_per_rec_cont = pickle.load(input_file)\n"
      ],
      "metadata": {
        "id": "RrZPsQcpeqqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and fit the model for three kinds of labelings\n",
        "# We do 10-fold cross validation\n",
        "k = 10\n",
        "mean_precision_cons, mean_recall_cons, mean_f1_cons, mean_support_cons, mean_auc_cons = xgb_model(X_consensus,y_consensus,k,number_of_samples_per_rec_cons)\n",
        "mean_precision_maj, mean_recall_maj, mean_f1_maj, mean_support_maj, mean_auc_maj = xgb_model(X_majority,y_majority,k,number_of_samples_per_rec_maj)\n",
        "mean_precision_cont, mean_recall_cont, mean_f1_cont, mean_support_cont, mean_auc_cont = xgb_model(X_contains,y_contains,k, number_of_samples_per_rec_cont)"
      ],
      "metadata": {
        "id": "qd_yjpHyerWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "\n",
        "print('Results for consensus labeling after {}-fold cross validation'.format(k))\n",
        "print('          precision --- recall --- f1-score --- support')\n",
        "print('Label 0:   ' + ' ' + str(mean_precision_cons[0])[0:5] + '       ' + str(mean_recall_cons[0])[0:5] + '       ' + str(mean_f1_cons[0])[0:5] + '        ' + str(mean_support_cons[0])[0:5])\n",
        "print('Label 1:   ' + ' ' + str(mean_precision_cons[1])[0:5] + '       ' + str(mean_recall_cons[1])[0:5] + '       ' + str(mean_f1_cons[1])[0:5] + '        ' + str(mean_support_cons[1])[0:4])\n",
        "print('AUC: {}\\n'.format(mean_auc_cons))\n",
        "\n",
        "print('Results for majority labeling after {}-fold cross validation'.format(k))\n",
        "print('          precision --- recall --- f1-score --- support')\n",
        "print('Label 0:   ' + ' ' + str(mean_precision_maj[0])[0:5] + '       ' + str(mean_recall_maj[0])[0:5] + '       ' + str(mean_f1_maj[0])[0:5] + '        ' + str(mean_support_maj[0])[0:5])\n",
        "print('Label 1:   ' + ' ' + str(mean_precision_maj[1])[0:5] + '       ' + str(mean_recall_maj[1])[0:5] + '       ' + str(mean_f1_maj[1])[0:5] + '        ' + str(mean_support_maj[1])[0:4])\n",
        "print('AUC: {}\\n'.format(mean_auc_maj))\n",
        "\n",
        "print('Results for contains labeling after {}-fold cross validation'.format(k))\n",
        "print('          precision --- recall --- f1-score --- support')\n",
        "print('Label 0:   ' + ' ' + str(mean_precision_cont[0])[0:5] + '       ' + str(mean_recall_cont[0])[0:5] + '       ' + str(mean_f1_cont[0])[0:5] + '        ' + str(mean_support_cont[0])[0:5])\n",
        "print('Label 1:   ' + ' ' + str(mean_precision_cont[1])[0:5] + '       ' + str(mean_recall_cont[1])[0:5] + '       ' + str(mean_f1_cont[1])[0:5] + '        ' + str(mean_support_cont[1])[0:4])\n",
        "print('AUC: {}\\n'.format(mean_auc_cont))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c67VlhfreuHe",
        "outputId": "5cbb4465-8bfd-48df-c78e-1c42ad966ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for consensus labeling after 10-fold cross validation\n",
            "          precision --- recall --- f1-score --- support\n",
            "Label 0:    0.968       0.851       0.905        7104.\n",
            "Label 1:    0.270       0.656       0.361        564.\n",
            "AUC: 0.8290790430455368\n",
            "\n",
            "Results for majority labeling after 10-fold cross validation\n",
            "          precision --- recall --- f1-score --- support\n",
            "Label 0:    0.961       0.817       0.882        7104.\n",
            "Label 1:    0.284       0.680       0.392        711.\n",
            "AUC: 0.8150817586111223\n",
            "\n",
            "Results for contains labeling after 10-fold cross validation\n",
            "          precision --- recall --- f1-score --- support\n",
            "Label 0:    0.927       0.776       0.843        7104.\n",
            "Label 1:    0.333       0.634       0.428        1160\n",
            "AUC: 0.7615874038793173\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RyPd7--Kwgx"
      },
      "source": [
        "# Neural Network hluti"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkucAw0qK0lO"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def eeg_split(recordings, y, number_of_samples_per_rec, random_seed):\n",
        "  \"\"\"\n",
        "  Splits the raw eeg data and labels into training, validation and testing sets on a patient basis.\n",
        "\n",
        "  Inputs: recordings is the raw eeg data\n",
        "          y is the corresponding labels\n",
        "          number_of_samples_per_rec is the number of samples from each recording so we can split the data on a patient basis\n",
        "\n",
        "  Output: eeg_train, eeg_test, eeg_val are the train, test and validation sets\n",
        "          y_train, y_test, y_val are the corresponding labels\n",
        "  \"\"\"\n",
        "  np.random.seed(random_seed)\n",
        "  testing_indiced = np.random.randint(0, 79, size=16)[:8]\n",
        "  val_indiced = np.random.randint(0, 79, size=16)[8:]\n",
        "\n",
        "  eeg_train = []\n",
        "  eeg_test = []\n",
        "  eeg_val = []\n",
        "  y_train = []\n",
        "  y_test = []\n",
        "  y_val = []\n",
        "\n",
        "\n",
        "  counter = 0\n",
        "  for i in range(79):\n",
        "    num = int(number_of_samples_per_rec[i])\n",
        "    if i in testing_indiced:\n",
        "      for j in range(num):\n",
        "        eeg_test.append(recordings[counter+j])\n",
        "        y_test.append(y[counter+j])\n",
        "    elif i in val_indiced:\n",
        "      for j in range(num):\n",
        "        eeg_val.append(recordings[counter+j])\n",
        "        y_val.append(y[counter+j])\n",
        "    else:\n",
        "      for j in range(num):\n",
        "        eeg_train.append(recordings[counter+j])\n",
        "        y_train.append(y[counter+j])\n",
        "    counter += num\n",
        "  return eeg_train, eeg_test, eeg_val, y_train, y_test, y_val\n",
        "\n",
        "def downsample(eeg_train, y_train):\n",
        "  \"\"\"\n",
        "  Downsamples the majority class of the training data\n",
        "\n",
        "  Inputs: eeg_train is the training data\n",
        "          y_train is the corresponding labels\n",
        "\n",
        "  Output: eeg_downsampled is the downsampled training data\n",
        "          y_downsampled is the corresponding labels\n",
        "  \"\"\"\n",
        "  majority_indices = [i for i, label in enumerate(y_train) if label == 0]\n",
        "  minority_indices = [i for i, label in enumerate(y_train) if label == 1]\n",
        "\n",
        "  random.seed(21)\n",
        "  downsampled_majority_indices = random.sample(majority_indices, len(minority_indices))\n",
        "\n",
        "  selected_indices = downsampled_majority_indices + minority_indices\n",
        "\n",
        "  random.shuffle(selected_indices)\n",
        "\n",
        "  eeg_downsampled = [eeg_train[i] for i in selected_indices]\n",
        "  y_downsampled = [y_train[i] for i in selected_indices]\n",
        "\n",
        "  return eeg_downsampled, y_downsampled"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Þetta downsamplar majority klasann í training settinu"
      ],
      "metadata": {
        "id": "11nBTJ_1TN66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the data\n",
        "eeg_train, eeg_test, eeg_val, y_train, y_test, y_val = eeg_split(recordings_consensus, y_consensus, number_of_samples_per_rec_consensus,21)\n",
        "eeg_downsampled, y_downsampled = downsample(eeg_train, y_train)\n",
        "\n",
        "# Turn into (n_samples, n_channels, n_measurements) np array and labels to (n_samples,) np arrays\n",
        "training_data = np.array(eeg_downsampled, dtype=np.float32)\n",
        "validation_data = np.array(eeg_val, dtype=np.float32)\n",
        "test_data = np.array(eeg_test, dtype=np.float32)\n",
        "training_label = np.array(y_downsampled, dtype=np.int32)\n",
        "validation_label = np.array(y_val, dtype=np.int32)\n",
        "test_label = np.array(y_test, dtype=np.int32)\n",
        "\n",
        "# We want the data on the format (n_samples,n_measurements, n_channels)\n",
        "training_data = np.transpose(training_data, (0, 2, 1))\n",
        "validation_data = np.transpose(validation_data, (0, 2, 1))\n",
        "test_data = np.transpose(test_data, (0, 2, 1))\n",
        "\n",
        "# Confirm the data shapes\n",
        "print('Training data shape {}'.format(training_data.shape))\n",
        "print('Training label shape {}'.format(training_label.shape))\n",
        "print('Validation data shape {}'.format(validation_data.shape))\n",
        "print('Validation label shape {}'.format(validation_label.shape))\n",
        "print('Test data shape {}'.format(test_data.shape))\n",
        "print('Test label shape {}'.format(test_label.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7U2oU31Wrwe",
        "outputId": "6db6f9d8-c61d-4070-9e3f-680235f809ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape (13516, 512, 21)\n",
            "Training label shape (13516,)\n",
            "Validation data shape (10767, 512, 21)\n",
            "Validation label shape (10767,)\n",
            "Test data shape (9145, 512, 21)\n",
            "Test label shape (9145,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline 1d conv network"
      ],
      "metadata": {
        "id": "0BnqYe2Wf2h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers, callbacks\n",
        "\n",
        "\n",
        "def ConvBlock(input_channels, output_channels, kernel_size):\n",
        "  \"\"\"\n",
        "  Creates a ConvBlock.\n",
        "  \"\"\"\n",
        "  return models.Sequential([\n",
        "        layers.Conv1D(filters=output_channels, kernel_size=kernel_size, padding=\"same\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU()\n",
        "    ])"
      ],
      "metadata": {
        "id": "8q9KdMNPf0st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def baseline_cnn_model():\n",
        "  \"\"\"\n",
        "  Creates a baseline CNN model.\n",
        "  \"\"\"\n",
        "  inputs = layers.Input(shape=(512,21))\n",
        "  conv_block = ConvBlock(1, 32, 16)(inputs)  # ConvBlock1\n",
        "  global_avg_pool = layers.GlobalAveragePooling1D()(conv_block)  # Global Average Pooling\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(global_avg_pool)\n",
        "\n",
        "  model = models.Model(inputs=inputs, outputs=outputs)\n",
        "  return model\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_reduction = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-6)\n",
        "\n",
        "model = baseline_cnn_model()\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\",\"auc\",\"recall\"])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "model.fit(training_data, training_label, validation_data=(validation_data, validation_label), epochs=10, batch_size=32, callbacks=[early_stopping, lr_reduction])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "0x38Vskyf8wa",
        "outputId": "8ec50a91-f07e-46d9-e596-809c0d9cf8da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_7 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m21\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ sequential_1 (\u001b[38;5;33mSequential\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │          \u001b[38;5;34m10,912\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling1d_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ sequential_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,912</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling1d_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,945\u001b[0m (42.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,945</span> (42.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,881\u001b[0m (42.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,881</span> (42.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m64\u001b[0m (256.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> (256.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - accuracy: 0.7464 - auc: 0.8406 - loss: 0.5734 - recall: 0.8618 - val_accuracy: 0.9838 - val_auc: 0.9183 - val_loss: 0.3646 - val_recall: 0.7222 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.8575 - auc: 0.9322 - loss: 0.4063 - recall: 0.8110 - val_accuracy: 0.9837 - val_auc: 0.9332 - val_loss: 0.3142 - val_recall: 0.7685 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 55ms/step - accuracy: 0.8770 - auc: 0.9488 - loss: 0.3417 - recall: 0.8332 - val_accuracy: 0.9811 - val_auc: 0.9093 - val_loss: 0.2982 - val_recall: 0.7426 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - accuracy: 0.8875 - auc: 0.9562 - loss: 0.3111 - recall: 0.8507 - val_accuracy: 0.8567 - val_auc: 0.9368 - val_loss: 0.4432 - val_recall: 0.8556 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 55ms/step - accuracy: 0.8944 - auc: 0.9580 - loss: 0.2962 - recall: 0.8621 - val_accuracy: 0.9104 - val_auc: 0.9075 - val_loss: 0.3360 - val_recall: 0.7870 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 69ms/step - accuracy: 0.8928 - auc: 0.9595 - loss: 0.2868 - recall: 0.8589 - val_accuracy: 0.8925 - val_auc: 0.9055 - val_loss: 0.3362 - val_recall: 0.8000 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 56ms/step - accuracy: 0.9085 - auc: 0.9696 - loss: 0.2521 - recall: 0.8806 - val_accuracy: 0.8327 - val_auc: 0.8969 - val_loss: 0.3756 - val_recall: 0.8000 - learning_rate: 5.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 59ms/step - accuracy: 0.9147 - auc: 0.9722 - loss: 0.2439 - recall: 0.8880 - val_accuracy: 0.8258 - val_auc: 0.8993 - val_loss: 0.3804 - val_recall: 0.8130 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fe2fe196a40>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "def model_eval(model, test_data, test_label):\n",
        "    \"\"\"\n",
        "    Evaluates a model on test_data.\n",
        "\n",
        "    Inputs:\n",
        "        model: The model to evaluate.\n",
        "        test_data: The data for evaluation.\n",
        "        test_label: The true labels (not one-hot encoded).\n",
        "    \"\"\"\n",
        "    predictions = model.predict(test_data)\n",
        "    predicted_labels = (predictions > 0.5).astype(int)\n",
        "    true_labels = test_label\n",
        "\n",
        "    # Print the classification report\n",
        "    report = classification_report(true_labels, predicted_labels, target_names=[\"Non seizure\", \"Seizure\"])\n",
        "    print(report)\n",
        "\n",
        "    # Print the AUC\n",
        "    auc_score = roc_auc_score(true_labels, predictions)\n",
        "    print(f\"Test AUC: {auc_score:.4f}\\n\")\n",
        "\n",
        "    # Print the confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "    print(f\"Confusion matrix:\\n{cm}\")\n"
      ],
      "metadata": {
        "id": "lYwxNbgugFTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval(model, test_data, test_label)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U_r70V9-jbVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple 2-dimensional convnet"
      ],
      "metadata": {
        "id": "e18b3lr7gLUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the data for 2d cnn\n",
        "training_data = training_data.reshape(training_data.shape[0],training_data.shape[1],training_data.shape[2],1)\n",
        "validation_data = validation_data.reshape(validation_data.shape[0], validation_data.shape[1], validation_data.shape[2], 1)\n",
        "test_data = test_data.reshape(test_data.shape[0],test_data.shape[1],test_data.shape[2],1)\n",
        "\n",
        "print('Training data shape {}'.format(training_data.shape))\n",
        "print('Training label shape {}'.format(training_label.shape))\n",
        "print('Validation data shape {}'.format(validation_data.shape))\n",
        "print('Validation label shape {}'.format(validation_label.shape))\n",
        "print('Test data shape {}'.format(test_data.shape))\n",
        "print('Test label shape {}'.format(test_label.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tWNl9lygG3e",
        "outputId": "307e4344-38c6-4819-ae44-437f163676fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape (13516, 512, 21, 1)\n",
            "Training label shape (13516, 2)\n",
            "Validation data shape (10767, 512, 21, 1)\n",
            "Validation label shape (10767, 2)\n",
            "Test data shape (9145, 512, 21, 1)\n",
            "Test label shape (9145, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We create a 2d cnn similar to our baseline 1d cnn\n",
        "input_shape = (512, 21, 1)\n",
        "\n",
        "model_a = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "    layers.MaxPooling2D((2, 2), padding='same'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_a.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy', 'auc'])\n",
        "\n",
        "# Model summary\n",
        "model_a.summary()\n",
        "\n",
        "# Train the model\n",
        "history_a = model_a.fit(training_data, training_label, epochs=10, batch_size=32,\n",
        "                        validation_data=(validation_data, validation_label))\n",
        "model_eval(model_a, test_data, test_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "id": "08vLY7FVgSLv",
        "outputId": "c059d06f-4d34-4226-bf22-c2ed9fa3d762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m320\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90112\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │         \u001b[38;5;34m180,226\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90112</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">180,226</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m180,546\u001b[0m (705.26 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">180,546</span> (705.26 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m180,546\u001b[0m (705.26 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">180,546</span> (705.26 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 291ms/step - accuracy: 0.6874 - auc: 0.7413 - loss: 0.7951 - val_accuracy: 0.8726 - val_auc: 0.9189 - val_loss: 0.4472\n",
            "Epoch 2/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 292ms/step - accuracy: 0.8477 - auc: 0.9234 - loss: 0.3641 - val_accuracy: 0.9169 - val_auc: 0.9484 - val_loss: 0.3328\n",
            "Epoch 3/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 339ms/step - accuracy: 0.8818 - auc: 0.9522 - loss: 0.2880 - val_accuracy: 0.9480 - val_auc: 0.9654 - val_loss: 0.2474\n",
            "Epoch 4/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 285ms/step - accuracy: 0.9188 - auc: 0.9752 - loss: 0.2111 - val_accuracy: 0.7897 - val_auc: 0.8523 - val_loss: 0.5648\n",
            "Epoch 5/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 295ms/step - accuracy: 0.9215 - auc: 0.9770 - loss: 0.1988 - val_accuracy: 0.8965 - val_auc: 0.9366 - val_loss: 0.3820\n",
            "Epoch 6/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 328ms/step - accuracy: 0.9489 - auc: 0.9900 - loss: 0.1374 - val_accuracy: 0.8721 - val_auc: 0.9282 - val_loss: 0.4159\n",
            "Epoch 7/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 333ms/step - accuracy: 0.9673 - auc: 0.9954 - loss: 0.0974 - val_accuracy: 0.9167 - val_auc: 0.9532 - val_loss: 0.3613\n",
            "Epoch 8/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 324ms/step - accuracy: 0.9661 - auc: 0.9957 - loss: 0.0908 - val_accuracy: 0.9375 - val_auc: 0.9637 - val_loss: 0.3117\n",
            "Epoch 9/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 278ms/step - accuracy: 0.9715 - auc: 0.9961 - loss: 0.0830 - val_accuracy: 0.8169 - val_auc: 0.8762 - val_loss: 0.6876\n",
            "Epoch 10/10\n",
            "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 334ms/step - accuracy: 0.9536 - auc: 0.9870 - loss: 0.1494 - val_accuracy: 0.8548 - val_auc: 0.9112 - val_loss: 0.6022\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 77ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " Non seizure       0.90      0.91      0.91      7880\n",
            "     Seizure       0.42      0.40      0.41      1265\n",
            "\n",
            "    accuracy                           0.84      9145\n",
            "   macro avg       0.66      0.66      0.66      9145\n",
            "weighted avg       0.84      0.84      0.84      9145\n",
            "\n",
            "Test AUC: 0.6453\n",
            "\n",
            "Confusion matrix:\n",
            "[[7168  712]\n",
            " [ 758  507]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final CNN"
      ],
      "metadata": {
        "id": "v2kZ3XZBGsX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import regularizers\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def improved_cnn_model():\n",
        "    \"\"\"\n",
        "    Function for our improved CNN model.\n",
        "\n",
        "    output: model is a keras cnn model\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=(512, 21))\n",
        "\n",
        "    x = layers.Conv1D(64, 16, padding='same', activation='relu',\n",
        "                      kernel_regularizer=regularizers.l2(0.001))(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling1D(2)(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = layers.Conv1D(128, 8, padding='same', activation='relu',\n",
        "                      kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling1D(2)(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = layers.Conv1D(256, 4, padding='same', activation='relu',\n",
        "                      kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "model = improved_cnn_model()\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', 'AUC', 'Recall', 'Precision'])\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_reduction = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-6)\n",
        "\n",
        "training_labels = training_label.argmax(axis=1)\n",
        "validation_labels = validation_label.argmax(axis=1)\n",
        "test_labels = test_label.argmax(axis=1)\n",
        "\n",
        "# Adjust the weights for better recall\n",
        "class_weights = {0: 1.0, 1: 3.0}\n",
        "\n",
        "history = model.fit(\n",
        "    training_data, training_labels,\n",
        "    validation_data=(validation_data, validation_labels),\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stopping, lr_reduction],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "y_test_probs = model.predict(test_data).ravel()\n",
        "y_val_probs = model.predict(validation_data).ravel()\n",
        "y_val_true = validation_labels\n",
        "precision, recall, thresholds = precision_recall_curve(y_val_true, y_val_probs)\n",
        "\n",
        "# Plot precision and recall against thresholds\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds, precision[:-1], label='Precision', linewidth=2)\n",
        "plt.plot(thresholds, recall[:-1], label='Recall', linewidth=2)\n",
        "plt.xlabel('Threshold', fontsize=14)\n",
        "plt.ylabel('Score', fontsize=14)\n",
        "plt.title('Precision and Recall vs Threshold', fontsize=16)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "desired_recall = 0.90\n",
        "recall_diff = recall - desired_recall\n",
        "threshold_indices = np.where(recall_diff >= 0)[0]\n",
        "if len(threshold_indices) > 0:\n",
        "    threshold_index = threshold_indices[-1]\n",
        "else:\n",
        "    threshold_index = np.argmin(np.abs(recall_diff))\n",
        "optimal_threshold = thresholds[threshold_index]\n",
        "print(f\"Optimal Threshold for Recall {desired_recall}: {optimal_threshold:.4f}\")\n",
        "\n",
        "y_test_pred = (y_test_probs >= optimal_threshold).astype(int)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(test_labels, y_test_pred, digits=4))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(test_labels, y_test_pred))\n",
        "print(f\"AUC: {roc_auc_score(test_labels, y_test_probs):.4f}\")\n",
        "\n",
        "\n",
        "fpr, tpr, roc_thresholds = roc_curve(test_labels, y_test_probs)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {roc_auc_score(test_labels, y_test_probs):.4f}', linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
        "plt.xlabel('False Positive Rate', fontsize=14)\n",
        "plt.ylabel('True Positive Rate', fontsize=14)\n",
        "plt.title('ROC Curve', fontsize=16)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xGDcS4QyeTYL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
